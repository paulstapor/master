 
%
% Chapter 3 of my master thesis:
% The first real chapter
%

\chapter{Algebraic Preliminaries}


\section{Linear Poisson structures in infinite dimensions}
\label{sec:chap3_LinearPoisson}

As we have seen before, there has already been done some work on how to 
strictly quantize Poisson structures on vector spaces. Star products of
exponential type on locally convex vector spaces were topologized by Stefan 
Waldmann in \cite{waldmann:2014a} and then investigated more closely by 
Matthias Sch√∂tz in \cite{schoetz:2014a}. Hence, as a the next step, we want 
to do linear Poisson structures on locally convex vector spaces. This 
will give a new big class of Poisson structures, which will be deformable 
in a strict way. Before we do so in the rest of this master thesis, we 
recall briefly some basics on linear Poisson structures.


We will always take a vector space $V$ and look at Poisson structures on 
the coordinates which are elements of the dual space $V^*$. In order to 
cover most of the physically interesting examples by our reflections, we 
will assume that $V$ is a locally convex vector space. Every finite-
dimensional vector space is normable and complete and hence locally convex, 
so it fits in this framework. It is clear what $V^*$ should be and there is 
just one interesting topology on it. For infinite-dimensional spaces, the 
situation is more delicate: we have to think about what coordinates should 
be and how a Poisson structure on them could look like. A priori, it is not 
clear which dual we should consider: the algebraic dual $V^*$ of all linear 
forms on $V$, or the topological dual $V'$ which contains just the 
continuous linear forms? Here, one could argue that only $V'$ is of real 
interest, since otherwise we would encounter the very strange effect of 
having discontinuous polynomials, and the aim of constructing a continuous 
star product on them seems somehow pointless. 
But even if we stick to $V'$, the question of the topology still remains: 
do we want to consider the weak or the strong topology there and why one of 
them should be more interesting. In any case, we have to choose a topology 
on this space. Once this is done, we have to think about a good notion of 
Poisson tensors in this context. However, we encounter quite a number of 
question, which have no trivial answer. For this reason, it is worth 
looking at some equivalent formulations of $Pol(V^*)$ in the finite-
dimensional case, since they may allow better generalizations.


Let $V$ be a finite dimensional vector space. Now, there is now question 
about the dual or its topology, since $V^* = V'$ is finite-dimensional, 
too, and we deal with polynomials on it. A linear Poisson structure on 
$V^*$ is something very familiar: it is equivalent to a Lie algebra 
structure on $V$.
\begin{proposition}
	\label{Alg:Prop:LinPoissonIsLieAlg}
	Let $V$ be a vector-space of dimension $n \in \mathbb{N}$ and $\pi \in 
	\Secinfty(\Anti^2(TV^*))$. Then the two following things are 
	equivalent:
	\begin{propositionlist}
		\item
		$\pi$ is a linear Poisson tensor.
		
		\item
		$V$ has a uniquely determined Lie algebra structure.
	\end{propositionlist}
\end{proposition}
\begin{proof}
	We choose a basis $e_1, \ldots, e_n \in V$ and denote its dual basis 
	$e^1, \ldots, e^n \in V^*$. Then we call the linear coordinates in 
	these bases $x_1, \ldots, x_n \in \Cinfty(V^*)$ and $\xi^1, \ldots, 
	\xi^n \in \Cinfty(V)$, such that for all $\xi \in V, x \in V^*$
	\begin{equation*}
		\xi
		=
		\xi^i(\xi) e_i
		\quad \text{ and } \quad
		x
		=
		x_i (x) e^i.
	\end{equation*}
	In these coordinates, the Poisson tensor reads
	\begin{equation*}
		\pi
		=
		\frac{1}{2}
		\pi_{ij}(x)
		\frac{\partial}{\partial x_i}
		\wedge
		\frac{\partial}{\partial x_j},
	\end{equation*}
	where $\pi$ is linear in the coordinates and we have
	\begin{equation*}
		\pi_{ij}(x)
		=
		c_{ij}^k x_k.
	\end{equation*}
	This equivalent to a tensor
	\begin{equation*}
		c
		=
		\frac{1}{2}
		c_{ij}^k 
		e_k \tensor e^i \wedge e^j
	\end{equation*}
	which gives for $f,g \in \Cinfty(V^*)$
	\begin{equation}
		\label{Alg:KksInCoordinates}
		\{f, g\} (x)
		=
		\pi(df, dg)(x)
		=
		x_k c_{ij}^k 
		\frac{\partial f}{\partial x_i}
		\frac{\partial g}{\partial x_j},
	\end{equation}
	using the identification $T^*V^* \cong V^{**} \cong V$.
	But now, the statement is obvious, since antisymmetry of $\pi$ means
	antisymmetry of the $c_{ij}^k$ in the indices $i$ and $j$ and the 
	Jacobi identity for the Poisson tensor gives
	\begin{equation}
		\label{Alg:JacobiInStructureConst}
		c_{ij}^\ell c_{\ell k}^m
		+
		c_{j k}^\ell c_{\ell i}^m
		+
		c_{ki}^\ell c_{\ell j}^m
		=
		0
	\end{equation}
	for all $i, j, k, m$, since it must be fulfilled for all smooth 
	functions. Vicely versa, \eqref{Alg:JacobiInStructureConst} ensures the 
	Jacobi identity of $\pi$ in \eqref{Alg:KksInCoordinates}. Hence the map
	\begin{equation}
		\label{Alg:LieBracketOfKks}
		[ \cdot, \cdot ]
		\colon
		V
		\times
		V
		\longrightarrow
		V
		\quad
		(e_i, e_j)
		\longmapsto
		c_{ij}^k e_k
	\end{equation}
	defines a Lie bracket, since the $c_{ij}^k$ are antisymmetric and 
	fulfil the Jacobi identity and are therefore structures constants. 
	Conversely, the structure constants of a Lie algebra on $V$ define a 
	Poisson tensor on $V^*$ via \eqref{Alg:KksInCoordinates}.
\end{proof}
A more detailed explanation of Poisson manifolds in general, their 
correspondence to Lie algebroids this special correspondence of linear Poisson 
structures ond a Vector space and Lie algebras can be found in the textbook of 
Waldmann \cite{waldmann:2007a} or in his lecture notes 
\cite{waldmann:2015a:script}. From now on, however, we will call the original 
vector space $\lie{g}$ instead of 
$V$ which is more intuitive for a Lie algebra. Since this kind of Poisson 
systems has a particular structure, there is a proper name for them.
\begin{definition}[Kirillov-Kostant-Souriau bracket]
	\label{Def:KKS}
	Let $\lie{g}$ be a finite-dimensional Lie algebra. Then the Poisson 
	bracket $\{ \cdot , \cdot \}_{KKS}$, which is given by 
	Proposition~\ref{Alg:Prop:LinPoissonIsLieAlg} on $\lie{g}^*$ is called 
	the Kirillov-Kostant-Souriau bracket.
\end{definition}



Proposittion~\ref{Alg:Prop:LinPoissonIsLieAlg} gives us a hint how we could
think of ''infinite-dimensionsal vector spaces with linear Poisson 
tensor'':We take $\lie{g}$ to be an infinite-dimensional Lie algebra, which 
gives something like a linear Poisson structure on $\lie{g}'$.
If, on the other hand, we chose directly $\lie{g}'$ to have a linear
Poisson tensor, we would get a Lie algebra structure on $\lie{g}''$.
Of course, we could think of ''just using this structure on $\lie{g}$'',
but in general, this will \emph{not} be closed: taking the Lie bracket of
$\xi, \eta \in \lie{g}$, we might drop out of $\lie{g}$ and have
$[\xi, \eta] \subseteq \lie{g}'' \backslash \lie{g}$. Usually, such
a behaviour will not be of physical interest, since the algebras of 
physical systems are usually closed objects and the double-dual is
not object of interest. This is why we will translate  the term
''linear Poisson structure on $\lie{g}^*$'' by  ''$\lie{g}$ is a Lie 
algebra'' in infinite dimensions and  from now on,we will restrict our 
observations to systems which can be described as such. Remark however, 
that, from a mathematical point of view, this is a choice and not a 
necessity and other choices would be possible.


The next task are the polynomials on $\lie{g}'$. As already mentioned, it 
is not easy to find a good generalization for them, since for a locally 
convex Lie algebra $\lie{g}$, even $\lie{g}'$ will be a huge vector space. 
Again, it is helpful to go back to the finite-dimensional case, where we 
have the following result:
\begin{proposition}
	\label{Alg:Prop:PolIsSym}
	Let $\lie{g}$ be a vector space of dimension $n \in \mathbb{N}$. Then 
	the algebras $\Sym^{\bullet}(\lie{g})$ and $\Pol^{\bullet}(\lie{g}^*)$ 
	are canonically isomorphic.
\end{proposition}
\begin{proof}
	Since this is a very well-known result, we just want to sketch the 
	proof briefly: Take a basis $e_1, \ldots, e_n$ of $\lie{g}$ and its 
	linear coordinates $x_1, \ldots, x_n \in \Cinfty(\lie{g}^*)$ with 
	$x_i(x) = e_i(x)$ for $x \in \lie{g}^*$. On homogeneous symmetric 
	tensors this yields the map
	\begin{equation*}
		\mathcal{J}
		\colon
		\Sym^{\bullet}(\lie{g})
		\longrightarrow
		\Pol^{\bullet}(\lie{g}^*),
		\quad
		e_1^{\mu_1} \ldots e_n^{\mu_n}
		\longmapsto
		\xi_1^{\mu_1} \ldots \xi_n^{\mu_n}.
	\end{equation*}
	From the construction, we see that this is an isomorphism, but note, 
	that we have used the identification $\lie{g}^{**} \cong \lie{g}$ via
	\begin{equation*}
		e_i(x)
		=
		\langle x, e_i \rangle.
	\end{equation*}
\end{proof}
The last identification we used in the last step will not work in both
directions any more, but just in one: we have a canonical injection
$\Sym^{\bullet}(\lie{g}) \subseteq \Pol^{\bullet}(\lie{g}')$, so every 
symmetric tensor still gives a polynomial. Anyway, this gives an idea 
how to avoid speaking about $\Pol(\lie{g}^*)$ and its topology: we 
restrict right from the beginning to $\Sym^{\bullet}(\lie{g})$.
For finite-dimensional systems, both points of view are equivalent,
but in infinite dimensions, this becomes a choice. However,
we have good reasons to think that this is enough: we get a closed 
and reasonably big subalgebra of of the polynomials. Moreover, 
the symmetric tensor algebra is defined on infinite-dimensional spaces 
exactly in the same way as on finite-dimensional ones,  and the 
construction is identical. 

So finally, we found a suitable way of speaking about our object of 
interest: We replace linear Poisson structures on $\Pol^{\bullet}
(\lie{g}^*)$ by $\Sym^{\bullet}(\lie{g})$. 




\section{The Gutt star product}
\label{sec:chap3_GuttStar}

The aim of this chapter is to endow the symmetric algebra, and hence the 
polynomial algebra, with a new, noncommutative product. This is possible in 
a very natural way, due to the Poincar\'e-Birkhoff-Witt theorem. It links 
the symmetric tensor algebra $\Sym^{\bullet}(\lie{g})$ of a Lie algebra 
$\lie{g}$ to its universal enveloping algebra $\algebra{U}(\lie{g})$.


\subsection{The universal enveloping algebra}
\label{subsec:chap3_UniversalEnvelopingAlgebra}

If $\algebra{A}$ is an associative algebra, one can construct a Lie algebra 
out of it by using the commutator
\begin{equation*}
	[a,b]
	=
	a \cdot b - b \cdot a
	, \quad
	a,b \in \algebra{A}.
\end{equation*}
This construction is functorial, since it doesn't only map associative 
algebras to Lie algebras, but also morphisms of the former to those of the 
latter. While constructing a Lie algebra out of an associative algebra is 
easy, the reversed process is more complicated, but also possible. Every 
Lie algebra $\lie{g}$ can be embedded into a particular associative 
algebra, known as the universal enveloping algebra $\algebra{U}(\lie{g})$, 
whichis uniquely determined (up to isomorphism) by the universal property: 
for every unital associative algebra $\algebra{A}$ and every homomorphism 
for Lie algebras $\phi\colon \lie{g} \longrightarrow \algebra{A}$ using the 
commutator on $\algebra{A}$, one gets a unital homomorphism of associative 
algebras $\Phi \colon \algebra{U}(\lie{g}) \longrightarrow \algebra{A}$ 
such that the following diagram commutes:
\begin{center}
    \begin{tikzpicture}
        \matrix (m)[
        matrix of math nodes,
        row sep=2.5em,
        column sep=8em
        ]
        {
          \mathcal{U}(\lie{g}) & \\
           & \algebra{A} \\
          \lie{g} &  \\
        };
        \draw
        [-stealth]
        (m-1-1) 	edge node 
        			[above] 
        			{$\Phi$}
        					(m-2-2)
        	(m-3-1)	edge node
        			[below]
        			{$\phi$}
        					(m-2-2)
        					
        			edge node
        			[left]
        			{$\iota$}
        					(m-1-1)
        	;
    \end{tikzpicture}
\end{center}
The proof of existence and uniqueness of the universal enveloping algebra 
can be found in every standard textbook on Lie theory like 
\cite{hilgert.neeb:2012a} or \cite{varadarajan:1974a}, and we won't do it 
here in detail. Just recall that existence is proven by an explicit 
construction: one takes the tensor algebra $\Tensor^{\bullet}(\lie{g})$ and 
considers the two-sided ideal
\begin{equation*}
	\mathfrak{I}
	=
	\langle \xi \tensor \eta - \eta \tensor \xi - [\xi, \eta] \rangle
	, \quad
	\forall_{\xi, \eta \in \lie{g}}
\end{equation*}
inside of it. Then one gets the universal enveloping algebra by the 
quotient
\begin{equation}
	\label{Alg:UnivEnvAlg}
	\algebra{U}(\lie{g})
	=
	\frac{\Tensor^{\bullet}(\lie{g})}{\mathfrak{I}}.
\end{equation}
To avoid confusion, we will always denote the multiplication in 
$\algebra{U}(\lie{g})$ by $\odot$, whereas the commutative product in 
$\Sym^{\bullet}(\lie{g})$ will be denoted without a sign.
It follows from this construction, that $\algebra{U}(\lie{g})$ is a 
filtered algebra
\begin{equation*}
	\algebra{U}(\lie{g})
	=
	\bigcup_{k \in \mathbb{N}}
	\algebra{U}^k(\lie{g})
	, \quad
	\algebra{U}^k(\lie{g})
	= 
	\Big\lbrace
		x 
		= 
		\sum_i
		\xi_1^i \odot \ldots \odot \xi_n^i
	\ \Big| \ 
		\xi_j^i \in \lie{g}
		, \
		1 \leq j \leq n,
		i \in I
	\Big\rbrace.
\end{equation*}
Generally, we just get a filtration, not a graded structure, since the 
ideal $\mathfrak{I}$ is not homogeneous in the symmetric degree. We only 
get a graded structure on $\algebra{U}(\lie{g})$, if and only if $\lie{g}$ 
was commutative. Then $\algebra{U}(\lie{g})$ is isomorphic to the symmetric
tensor algebra and hence commutative, too. But $\algebra{U}(\lie{g})$
is much more than an associative algebra: it is also a Hopf algebra, 
since one can define a coassociative, cocommutative coproduct on it
\begin{equation*}
	\Delta \colon
	\algebra{U}(\lie{g})
	\longrightarrow
	\algebra{U}(\lie{g})
	\tensor
	\algebra{U}(\lie{g})
	, \quad
	\xi
	\longmapsto
	\xi \tensor \Unit
	+
	\Unit \tensor \xi
	, \quad
	\forall_{\xi \in \lie{g}}
\end{equation*} 
which extends to $\algebra{U}(\lie{g})$ via algebra homomorphism, 
as well as an antipode
\begin{equation*}
	S \colon
	\algebra{U}(\lie{g})
	\longrightarrow
	\algebra{U}(\lie{g})
	, \quad
	\xi
	\longmapsto
	- \xi
	, \quad
	\forall_{\xi \in \lie{g}}
\end{equation*}
which extends to $\algebra{U}(\lie{g})$ via algebra antihomomorphism.
We will come back to those two maps and to the Hopf structure in chapter 7,
when we talk about their continuity. More details on the algebraic
aspect of deformation theory using Hopf algebras can be found in 
\cite{chari.pressley:1994a} and \cite{majid:1995a}, for example.





\subsection{The Poincar\'e-Birkhoff-Witt theorem}
\label{subsec:chap3_PoincareBirkhoffWitt}

The algebra $\algebra{U}(\lie{g})$ always admits a basis, which must be 
infinite. This result is due to the already mentioned theorem of 
Poincar\'e, Birkhoff and Witt:
\begin{theorem}[Poincar\'e-Birkhoff-Witt theorem]
	\label{Thm:Alg:PBW}
	Let $\lie{g}$ be a Lie algebra with a basis $\mathcal{B}_{\lie{g}} = \{ 
	\beta_i \}_{i \in I}$. Then the set
	\begin{equation*}
		\mathcal{B}_{\algebra{U}(\lie{g})}
		=
		\big\{
			\beta_{i_1}^{\mu_{i_1}}
			\odot \ldots \odot
			\beta_{i_n}^{\mu_{i_n}}
		\ \big| \
			n \in \mathbb{N}, \ 
			i_k \in I
			\text{ with } i_1 \earlier \ldots \earlier i_n 
			\text{ and } \beta_{i_k} \in \mathcal{B}_{\lie{g}}, \
			\mu_{i_1}, \ldots, \mu_{i_n} \in \mathbb{N}
		\big\}
	\end{equation*}
	defines a basis of $\algebra{U}(\lie{g})$.
\end{theorem}
There are different proofs for this statement. While a geometrical proof 
(like it can be found in \cite{waldmann:2015a:script}) is very convenient 
in the finite-dimensional case, a combinatorial argument must be used for 
infinite-dimensional Lie algebras. Most textbooks restrict to 
finite-dimensional Lie algebras and give a version of the latter one, except 
\cite{bourbaki:1989a}, which does it ni full generality. The idea of most
of the combinatoric proof works with minor changes also for any Lie algebra,
since in relies on ordered index sets which can be defined in any dimension.
The PBW theorem allows us to set up an isomorphism between $\Sym^{\bullet}
(\lie{g})$ and $\algebra{U}(\lie{g})$ immediately, since a basis of the 
former can be given by almost the same expression
\begin{equation*}
	\mathcal{B}_{\Sym^{\bullet}(\lie{g})}
	=
	\big\{
		\beta_{i_1}^{\mu_{i_1}}
		\cdots
		\beta_{i_n}^{\mu_{i_n}}
	\ \big| \
		n \in \mathbb{N}, \ 
		i_k \in I, 1 \leq k \leq n,
		i_1 \earlier \cdots \earlier i_n 
		\text{ and } \beta_{i_k} \in \mathcal{B}_{\lie{g}}, \
		\mu_{i_1}, \ldots, \mu_{i_n} \in \mathbb{N}
	\big\}
\end{equation*}
where we just have replaced the noncommutative product in $\algebra{U}
(\lie{g})$ by the symmetric tensor product $\cdot$ (which we will usually 
denote without a symbol, if possible) in $\Sym^{\bullet}(\lie{g})$. This 
allows us to write down an isomorphism between the symmetric tensor algebra 
and the universal enveloping algebra, just by mapping the basis vectors to 
each other in a naive way. Of course, this can never be an isomorphism in 
the sense of algebras, but only of (filtered) vector spaces, because one of 
the algebras is commutative and the other isn't. Moreover, the symmetric 
algebra has a graded structure from the one on the tensor algebra which the 
universal enveloping algebra does not have:
\begin{equation*}
	\Sym^{\bullet}(\lie{g})
	=
	\bigoplus\limits_{n = 0}^{\infty}
	\Sym^n(\lie{g})
	, \quad
	\Sym^n(\lie{g})
	=
	\underbrace{
		\lie{g} \vee \ldots \vee \lie{g}
	}_{
		n \text{ times}
	}.
\end{equation*}
We will denote by $\pi_n \colon \Sym^{\bullet}(\lie{g}) \longrightarrow
\Sym^n(\lie{g})$ the canonical projections of this grading.
This induces a filtration by $\Sym^{(k)}(\lie{g}) = \sum_{j=0}^k 
\Sym^j(\lie{g})$. Our simple isomorphism will respect the filtration, but 
not the grading. However, it isn't the only isomorphism which one can write 
down. In \cite{berezin:1967a}, Berezin proposed another isomorphism which is 
more helpful to use:
\begin{equation}
	\label{Alg:BerezinQuantization}
	\mathfrak{q}_n
	\colon
	\Sym^n(\lie{g})
	\longrightarrow
	\algebra{U}^n(\lie{g})
	, \quad
	\beta_{i_1} \ldots \beta_{i_n}
	\longmapsto
	\frac{1}{n!}
	\sum\limits_{\sigma \in S_n}
	\beta_{i_{\sigma(1)}} 
	\cdot \ldots \cdot
	\beta_{i_{\sigma(n)}}
	, \quad
	\mathfrak{q}
	=
	\sum\limits_{n = 0}^{\infty}
	\mathfrak{q}_n.
\end{equation}
We will refer to it as the quantization map, for reasons that will soon 
become clear. It also respects the filtration and transfers the symmetric 
product to another symmetric expression. In this sense, we can now switch 
between both algebras and use the setting, which is more convenient in the 
current situation: the graded structure of $\Sym^{\bullet}(\lie{g})$, or the 
Hopf algebra structure of $\algebra{U}(\lie{g})$.


\subsection{The Gutt star product}
Since we know, that the universal enveloping and the symmetric tensor 
algebra are isomorphic as vector spaces, we have a good tool at hand to 
endow the symmetric tensor algebra, and hence the polynomials, with a 
noncommutative product by pulling back the product from $\algebra{U}
(\lie{g})$ to $\Sym^{\bullet}(\lie{g})$ via $\mathfrak{q}$. This is exactly 
what Gutt did in \cite{gutt:1983a}. She constructed a star product on 
$\Pol^{\bullet}(\lie{g}^*)$ from $\algebra{U}(\lie{g})$ while encoding the 
noncommutativity in a formal parameter $z \in \mathbb{C}$ in a convenient 
way. 
\begin{definition}[Gutt star product]
	\label{Def:GuttStar}
	Let $\lie{g}$ be a Lie algebra, $z \in \mathbb{C}$, and 
	$f, g \in \Sym^{\bullet}(\lie{g})$ of degree $k$ and $\ell$ 
	respectively. Then we define the Gutt star product by:
	\begin{equation}
		\star_z
		\colon
		\Sym^{\bullet}(\lie{g})
		\times
		\Sym^{\bullet}(\lie{g})
		\longrightarrow
		\Sym^{\bullet}(\lie{g})
		, \quad
		(f,g)
		\longmapsto
		\sum\limits_{n = 0}^{k + \ell - 1}
		z^n
		\pi_{k + \ell - n}
		\left(
			\mathfrak{q}^{-1} \left(
				\mathfrak{q}(f) \cdot \mathfrak{q}(g)
			\right)
		\right).
	\end{equation}
\end{definition}
This is the original way in which Gutt defined her star product in 
\cite{gutt:1983a}, but there are two more ways to do it. Define
\begin{equation*}
	\mathfrak{I}_z
	=
	< \xi \tensor \eta - \eta \tensor \xi - z [\xi, \eta] >
\end{equation*}
for $z \in \mathbb{C}$. Then we set 
\begin{equation}
	\label{Alg:DeformedUnivEnvAlg}
	\algebra{U}(\lie{g}_z)
	=
	\frac{\Tensor^{\bullet}(\lie{g})}{\mathfrak{I}_z},
\end{equation}
and get the map
\begin{equation}
	\label{Alg:DeformedQuantization}
	\mathfrak{q}_{z,n}
	\colon
	\Sym^n(\lie{g})
	\longrightarrow
	\algebra{U}^n(\lie{g}_z)
	, \quad
	\beta_{i_1} \ldots \beta_{i_n}
	\longmapsto
	\frac{1}{n!}
	\sum\limits_{\sigma \in S_n}
	\beta_{i_{\sigma(1)}} 
	\cdot \ldots \cdot
	\beta_{i_{\sigma(n)}}
	, \quad
	\mathfrak{q}_z
	=
	\sum\limits_{n = 0}^{\infty}
	\mathfrak{q}_{z, n}.
\end{equation}
This way, we also get a star product:
\begin{equation}
	\label{Alg:DeformedStar}
	\widehat{\star}_z
	\colon
	\Sym^{\bullet}(\lie{g})
	\times
	\Sym^{\bullet}(\lie{g})
	\longrightarrow
	\Sym^{\bullet}(\lie{g})
	, \quad
	(f,g)
	\longmapsto
	\mathfrak{q}_z^{-1} 
	\left(
		\mathfrak{q}_z(f) \cdot \mathfrak{q}_z(g)
	\right).
\end{equation}
In \cite{drinfeld:1983a}, Drinfeld also constructed a star product using the 
Baker-Campbell-Hausdorff series: take $\xi, \eta \in \lie{g}$ and set
\begin{equation}
	\label{Alg:DrinfeldStar}
	\exp(\xi) \ast_z \exp(\eta)
	=
	\exp \left(
		\frac{1}{z}
		\bch{z \xi}{z \eta}
	\right),
\end{equation}
where the exponential series is understood a formal power series in $\xi$ 
and $\eta$. By formally differentiating, on gets the star product on all 
polynomials.


Of course, our aim is to show that these three maps are in fact identical 
and that they define a star product. Since this is a long way to go, we 
postpone the proof to the end of this chapter. It will be useful to learn 
something about the Baker-Campbell-Hausdorff series and the Bernoulli number 
first.



\section{The Baker-Campbell-Hausdorff series}
\label{sec:chap3_BCH}

Since we have a formula for $\star_z$ which involves the
Baker-Campbell-Hausdorff series, we want to give a short overview
about it and introduce some results that will be helpful later on.
Note however, that there is not \emph{the} BCH \emph{formula}, since one 
can always rearrange terms using antisymmetry or Jacobi identity, but for 
$\xi, \eta \in \lie{g}$, we can always write it as
\begin{equation}
	\label{Alg:NamesOfBCH}
	\bch{\xi}{\eta}
	=
	\sum\limits_{n = 1}^{\infty}
	\bchpart{n}{\xi}{\eta}
	=
	\sum\limits_{a,b = 0}^{\infty}
	\bchparts{a}{b}{\xi}{\eta},
\end{equation}
where $\bchpart{n}{\xi}{\eta}$ denotes all expressions having $n$ letters 
and $\bchparts{a}{b}{\xi}{\eta}$ denotes all expressions with $a$ $\xi$'s 
and $b$ $\eta$'s. We have $\bchparts{0}{0}{\xi}{\eta} = 0$, $\bchparts{1}{0}
{\xi}{\eta} = \xi$ and $\bchparts{0}{1}{\xi}{\eta} = \eta$. Clearly this 
gives
\begin{equation*}
	\bchpart{n}{\xi}{\eta}
	=
	\sum\limits_{a + b = n}
	\bchparts{a}{b}{\xi}{\eta}.
\end{equation*}
Of course, this only moves the problem of non-uniqueness to a later point 
when we will have to discuss the partial expressions. Yet, in the beginning, 
this will be helpful.



\subsection{Some general and historical remarks}
Assume $\lie{g}$ to be the Lie algebra of a finite-dimensional Lie group 
$G$. From the geometric point of view, the BCH formula is the infinitesimal 
counterpart of the multiplication law in $G$. Since the multiplication is 
smooth and and the exponential function locally diffeomorphic around the 
unit element $e$, we would expect that there is a Lie algebraic analogon to 
the group multiplication, at least near the origin, which depends somehow 
''smoothly on the arguments''. Finding this expression is, however, a 
different task.


One approach to this would be the following: consider an algebra 
$\algebra{A}$ with the noncommuting elements $\xi, \eta$. We want to study 
the identity of formal power series 
\begin{equation*}
	\exp(\chi)
	=
	\exp(\xi)
	\exp(\eta).
\end{equation*}
There should be a $\chi \in \algebra{A}$, which fulfils this relation. 
We can rewrite the right hand side as
\begin{equation*}
	\exp(\xi)\exp(\eta) 
	= 
	\sum\limits_{n,m=0}^{\infty} 
	\frac{\xi^n \eta^m}{n! m!}
\end{equation*}
and use the formal power series for the logarithm
\begin{equation*}
	\log(\chi) 
	= 
	\sum\limits_{k = 1}^{\infty} 
	\frac{ (-1)^{k-1} }{ k } (\chi - 1)^k
\end{equation*}
in order to get an expression for $\chi$. This yields
\begin{equation}
	\label{Alg:BCHinRaw}
	\chi
	= 
	\log \left( \exp(\xi) \exp(\eta) \right) 
	= 
	\sum\limits_{k = 1}^{\infty} 
	\frac{ (-1)^{k - 1} }{k} 
	\sum\limits_{ \substack{ 
		i \in \{1, \ldots, k\} \\ 
		n_i, m_i \geq 0 \\ n_i + m_i \geq 1
	}}
	\frac{
		\xi^{n_1} \eta^{m_1} 
		\ldots 
		\xi^{n_k} \eta^{m_k}
	}
	{n_1! m_1! \ldots n_k! m_k!}.
\end{equation}
It is far from trivial, if and how this can be expressed using Lie brackets.
The first one who found a general way for this was Dynkin in the 1950's 
\cite{dynkin:1947a, dynkin:1950a}. Of course, the question of convergence 
still remains, although we would expect the expression to converge at least in 
a neighbourhood of $0$.


A different approach works via differential equations. We can consider flows 
on the Lie group. This gives also an expression of the group multiplication 
in logarithmic coordinates just using Lie brackets. One gets recursive 
relations for the $\bchpart{n}{\xi}{\eta}$ and the first formulas due to 
Baker \cite{baker:1905a}, Campbell \cite{campbell:1896a, campbell:1897a} and 
Hausdorff \cite{hausdorff:1906a} were of this kind. For the first terms, one 
finds
\begin{align}
	\nonumber
	\bch{\xi}{\eta}
	& =
	\log \left( \exp(X) \exp(Y) \right)
	\\
	\nonumber
	& =
	\xi + \eta
	+ \frac{1}{2} [\xi, \eta]
	+ \frac{1}{12} ( [[\eta, \xi], \xi]  + [[\xi, \eta], \eta] )
	+ \frac{1}{24} [[[\eta, \xi], \xi], \eta]
	\\
	\nonumber
	& \quad
	+ \frac{1}{120}
	(
		[[[[\eta, \xi], \eta], \xi], \eta] + 
		[[[[\xi, \eta], \xi], \eta], \xi]
	)
	+ \frac{1}{360}
	(
		[[[[\eta, \xi], \xi], \xi], \eta] + 
		[[[[\xi, \eta], \eta], \eta], \xi]
	)
	\\
	\label{Alg:BCHSeriesLong}
	& \quad
	- \frac{1}{720}
	(
		[[[[\eta, \xi], \xi], \xi], \xi] + 
		[[[[\xi, \eta], \eta], \eta], \eta]
	)
	+ \cdots
\end{align}
which coincides of course with the result from Dynkin.


\subsection{Forms of the BCH}
As already mentioned, there are different forms of stating the BCH formula 
and depending on the problem one wants to solve, not every one is equally 
well suited. One can classify them roughly into four groups.
\begin{enumerate}
	\item
	There are recursive formulas, which calculate each term from the 
	previous one. The first expressions due to Baker, Campbell and Hausdorff 
	were of this kind. Though the idea is old, this approach is still much in
	use and allows powerful applications: Casas and Murua found an 
	efficient algorithm \cite{casas.murua:2009a} for calculating 	a form of 
	BCH series without redundancies based on a recursive formula, which was 
	given by Varadarajan in his textbook \cite{varadarajan:1974a}. 
	For such a non-redundant formula one needs a notion of basis of the free 
	Lie algebra. There are approaches to such (Hall or Hall-Viennot) basis, 
	which can e.g. be found in \cite{serre:2006a}.
	
	\item
	Most textbooks prove an integral form of the series, like 
	\cite{hall:2003a} and \cite{hilgert.neeb:2012a}. Since we will use it, 
	too, we want to introduce it briefly. Take the function
	\begin{equation}
		\label{Alg:DefinitionLogBernoullis}
		g \colon 
		\mathbb{C}
		\longrightarrow
		\mathbb{C},
		\quad
		z 
		\longmapsto
		\frac{ z \log(z) }{z - 1}
	\end{equation}
	and denote for $\xi \in \lie{g}$ by
	\begin{equation*}
		\ad_{\xi}
		\colon
		\lie{g}
		\longrightarrow
		\lie{g},
		\quad
		\eta
		\longmapsto
		[\xi, \eta]
	\end{equation*}
	the usual ad-operator. Then one has for $\xi, \eta \in \lie{g}$
	\begin{equation}
		\label{Alg:BCHinIntegral}
		\bch{\xi}{\eta}
		=
		\xi + 
		\int\limits_0^1
		g \left( 
			\exp \left(
				\ad_{\xi}
			\right)
			\exp \left(
				t \ad_{\eta}
			\right)
		\right)
		(\eta) 
		dt.
	\end{equation}
	
	\item
	As already mentioned, Dynkin found a closed form for \eqref{Alg:BCHinRaw},
	which is the only one of this kind known so far. A proof can be found in 
	\cite{jacobson:1979a}, for example. It reads
	\begin{equation}
		\label{Alg:BCHinDynkin}
		\bch{\xi}{\eta}
		=
		\sum\limits_{k = 1}^{\infty} 
		\frac{ (-1)^{k - 1} }{k}
		\sum\limits_{\substack{
			i \in \{1, \ldots, k\} \\
			n_i, m_i \geq 0 \\ 
			n_i + m_i \geq 1
		}}
		\frac{1}{ \sum_{i = 1}^k(n_i + m_i) }
		\frac{ \left[
			\xi^{n_1} \eta^{m_1} \ldots \xi^{n_k} \eta^{m_k}
		\right]}
		{ n_1! m_1! \ldots n_k! m_k! },
	\end{equation}
	where the expression $[ \ldots ]$ denotes Lie brackets nested to the left:
	\begin{equation*}
		[\xi \eta \eta \xi]
		=
		[[[\xi, \eta], \eta], \xi].
	\end{equation*}
	Unfortunately, the combinatorics get extremely complicated for higher 
	degrees and increasingly many terms belong to the same Lie bracket 
	expression.
	
	\item
	Goldberg gave a form of the series which is based on words in two letters:
	\begin{equation}
		\bch{\xi}{\eta}
		=
		\sum\limits_{n = 1}^{\infty}
		\sum\limits_{|w| = n}
		g_w w.
	\end{equation}
	The $g_w$ are coefficients, which can be calculated using the recursively
	defined Goldberg polynomials (see \cite{goldberg:1956a}). It was put into 
	commutator form by Thompson in \cite{thompson:1982a}:
	\begin{equation}
		\label{Alg:BCHinGoldbergThompson}
		\bch{\xi}{\eta}
		=
		\sum\limits_{n = 1}^{\infty}
		\sum\limits_{|w| = n}
		\frac{g_w}{n} [w].
	\end{equation}
	Again, the $[w]$ are Lie brackets nested to the left. Of course, this 
	formula will also have redundancies, but its combinatorial aspect is much 
	easier than the one of \eqref{Alg:BCHinDynkin}. Since there are estimates 
	for the coefficients $g_w$, we will use this form for our Main Theorem.
\end{enumerate}


\subsection{The Goldberg-Thompson formula and some results}

\subsubsection{Goldberg's theorems}
We now introduce the results of Goldberg: he noted a word in the letters $\xi$ 
and $\eta$ as
\begin{equation*}
	w_{\xi}(s_1, s_2, \ldots, s_m)
	=
	\xi^{s_1} \eta^{s_2} \ldots (\xi \vee \eta)^{s_m},
\end{equation*}
with $m \in \mathbb{N}$ and the last letter will be $\xi$ if $m$ is odd and 
$\eta$ if $m$ is even. The index $\xi$ of $w_{\xi}$ means that the word 
starts with a $\xi$. Now we can assign to each word $w_{\xi \vee \eta}(s_1, 
\ldots, s_m)$ a coefficient $c_{\xi \vee \eta}(s_1, \ldots, s_m)$. This is 
done by the following formula:
\begin{equation}
	\label{Alg:GoldbergCoeff1}
	c_{\xi}(s_1, \ldots, s_m) 
	= 
	\int\limits_0^1 
	t^{m'} (t - 1)^{m''} 
	G_{s_1}(t) \ldots G_{s_m}(t) dt,
\end{equation}
where we have $m' = \lfloor \frac{m}{2} \rfloor$, $m'' = \lfloor \frac{m-1}{2} 
\rfloor$ with $\lfloor \cdot \rfloor$ denoting the entire part of a real 
number and we have $n = \sum_{i = 1}^m s_i$. The $G_s$ are the recursively 
defined Goldberg polynomials
\begin{equation}
	\label{Alg:GoldbergPolynomials}
	G_s(t) 
	= 
	\frac{1}{s} 
	\frac{d}{dt} 
	t(t-1) G_{s-1}(t),
\end{equation}
for $s > 1$ and $G_1(t) = 1$. For $c_{\eta}$ we have
\begin{equation}
	\label{Alg:GoldbergCoeff2}
	c_{\eta}\left( s_1, \ldots, s_m \right)
	= 
	(-1)^{n-1} c_{\xi} \left( s_1, \ldots, s_m \right)
\end{equation}
and furthermore
\begin{equation*}
	c_{\eta}\left( s_1, \ldots, s_m \right)
	= 
	c_{\xi} \left( s_1, \ldots, s_m \right)
\end{equation*}
if $m$ is odd. This yields immediately
\begin{equation*}
	c_{\xi}\left( s_1, \ldots, s_m \right)
	= 
	c_{\eta}\left( s_1, \ldots, s_m \right) 
	= 
	0 
\end{equation*}
if $m$ is odd and $n$ is even. Of course, Goldberg found interesting identities 
which are fulfilled by the coefficients. A very remarkable one is that for all 
permutations $\sigma \in S_m$ one has
\begin{equation*}
	c_{\xi}\left( s_1, \ldots, s_m \right)
	=
	c_{\xi}\left( s_{\sigma(1)}, \ldots, s_{\sigma(m)} \right),
\end{equation*}
since \eqref{Alg:GoldbergCoeff1} obviously doesn't see the ordering of the 
$s_i$ and $m'$, $m''$ and $n$ are not affected by reordering. For words with 
$m = 2$, an easier formula can be found:
\begin{equation*}
	c_{\xi}(s_1, s_2) 
	= 
	\frac{ (-1)^{s_1} }{s_1! s_2!} 
	\sum\limits_{n = 1}^{s_2} 
	\binom{s_2}{n} B_{s_1 + s_2 - n},
\end{equation*}
where the $B_s$ denote the Bernoulli numbers, which will be explained more 
precisely in the next paragraph. First, we note that the only case which 
matters to us is of course $s_1 = 1$, since for $s_1, s_2 > 1$ we will find 
something like $[[\xi, \xi], \ldots ] = 0$. For simplicity, let's set 
$s_2 = 1$ and to permute $s_1 \leftrightarrow s_2$:
\begin{equation}
	\label{Alg:GoldbergCoeffBernoulli}
	c_{\xi}(1, s)
	=
	\frac{ (-1)^s }{s!}
	B_s.
\end{equation}


\subsubsection{Bernoulli numbers}
We have seen the Bernoulli numbers $B_n$ showing up and we will encounter them 
very often in the following. Hence it is useful to learn a few important 
things about them. They are defined by the series expansion of
\begin{equation}
	\label{Alg:DefBernoulli}
	g \colon
	\mathbb{C}
	\longrightarrow
	\mathbb{C}
	, \quad
	z \longmapsto
	\frac{z}{\E^z - 1}
	=
	\sum\limits_{n=0}^{\infty}
	\frac{B_n}{n!}z^n.
\end{equation}
Clearly, $g$ has poles at $z = 2 k \pi i$, $k \in \mathbb{Z}\backslash \{0\}$.
Moreover, one can easily show that all odd Bernoulli numbers are zero, except 
for $B_1 = - \frac{1}{2}$ and since in some applications one wants $B_1$ to be 
positive, there is a different convention for naming them: one often 
encounters $B_n^* = (-1)^n B_n$ (which only differs for $n = 1$). The nonzero 
Bernoulli numbers alternate in sign. For their absolute value, one can show 
the asymptotic behaviour (see \cite{oeis:A027641, oeis:A027642})
\begin{equation*}
	|B_{2n}|
	\sim 
	(-1)^{n-1}
	\frac{2 (2n)!}{(2 \pi)^{2*n}}.
\end{equation*}
This is not surprising, since we know that the generating function $g$ had 
poles at $\pm 2\pi i$. The Bernoulli numbers can also be calculated by the 
recursion formula
\begin{equation}
	\label{Alg:BernoulliRecursive}
	B_n
	=
	- \frac{1}{n + 1}
	\sum\limits_{k = 0}^{n- 1}
	\binom{n + 1}{k}
	B_k,
\end{equation}
which is well-known in the literature (e.g. 
\cite{arakawa.ibukiyama.kaneko:2014a}). Since we will deal with them, 
we want to give the first numbers of this series here.
\begin{center}
	\begin{tabular}
	{c||c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
		$n$ & $0$ & $1$ & 
		$2$ & $3$ & $4$ & 
		$5$ & $6$ & $7$ & 
		$8$ & $9$ & $10$ & 
		$11$ & $12$ & $13$ & 
		$14$ & $15$ & $16$
		\\
		\hline 
		$B_n$ & $1$ & $-\frac{1}{2}$ & 
		$\frac{1}{6}$ & $0$ & $-\frac{1}{30}$ & 
		$0$ & $\frac{1}{42}$ & $0$ & 
		$-\frac{1}{30}$ & $0$ & $\frac{5}{66}$ & 
		$0$ & $\frac{691}{2730}$ & $0$ & 
		$\frac{7}{6}$ & $0$ & $\frac{3617}{510}$
	\end{tabular} 
\end{center}


\subsubsection{BCH up to first order}

\begin{proposition}
	\label{Alg:Prop:BCHFristOrder}
	Let $\lie{g}$ be a Lie algebra and the Bernoulli numbers as defined 
	before. Then we have for $\xi, \eta \in \lie{g}$
	\begin{align}
		\label{Alg:BCHFirstOrderXi}
		\bch{\xi}{\eta}
		& =
		\sum\limits_{n = 0}^{\infty}
		\frac{B_n^*}{n!}
		\left( \ad_{\xi} \right)^n (\eta)
		+ \mathcal{O} \left( \eta^2 \right)
		\\
		\label{Alg:BCHFirstOrderEta}
		& =
		\sum\limits_{n = 0}^{\infty}
		\frac{B_n}{n!}
		\left( \ad_{\eta} \right)^n (\xi)
		+ \mathcal{O} \left( \xi^2 \right)
	\end{align}
\end{proposition}
\begin{proof}
	We want to can calculate this using the Goldberg coefficients. Remind that 
	we will put words to Lie brackets, and for computing the coefficients we 
	will need the words $\eta \xi^n$ \emph{and} $\xi \eta \xi^{n-1}$ because 
	of antisymmetry and words of the form $\xi^k \eta \xi^{n-k}$ with $k > 1$ 
	will give vanishing expressions. Now let $n \in \mathbb{N}$. We have
	\begin{equation*}
		c_{\eta}(1,n) 
		= 
		(-1)^n c_{\xi}(1,n) 
		= 
		(-1)^n 
		\frac{(-1)^n}{n!} 
		B_n 
		= 
		\frac{B_n}{n!}.
	\end{equation*}
	By $n$-fold skew-symmetry and \eqref{Alg:BCHinGoldbergThompson}, we get 
	the contribution
	\begin{equation*}
		\frac{(-1)^n}{(n+1)!} 
		B_n \left( \ad_{\xi} \right)^n (\eta)
		=
		\frac{1}{(n+1)!} 
		B_n^* \left( \ad_{\xi} \right)^n (\eta)		
	\end{equation*}
	Now we need $c_{\xi}(1, 1, n-1)$: let $n > 1$, then
	\begin{align*}
		c_{\xi}(1, 1, n - 1) 
		& = 
		\int\limits_0^1 
		t(t-1) G_{n-1}(t) 
		dt 
		\\
		& = 
		- \int\limits_0^1 
		t \frac{d}{dt} 
		\left( t(t-1) G_{n-1}(t) \right) 
		dt 
		\\
		& =
		- \int\limits_0^1 
		n t G_n(t) 
		dt 
		\\
		& = 
		-n 
		c_{\xi}(1,n) 
		\\
		& = 
		-n 
		\frac{(-1)^n}{n!} 
		B_n 
		\\
		& =  
		(-1)^{n + 1} 
		\frac{1}{(n-1)!} B_n,
	\end{align*}
	where we have done an integration by parts in the third step.
	So by using $n-1$ times the skew-symmetry of the Lie bracket, we get
	\begin{equation*}
		\frac{1}{n + 1} 
		\cdot (-1)^{n + 1} 
		\frac{1}{ (n - 1)! }
		B_n 
		[\ldots [\xi, \eta], \xi] \ldots ], \xi] 
		= 
		\frac{n}{(n+1)!} 
		B_n 
		\left( \ad_{\xi} \right)^n (\eta)
	\end{equation*}
	For $n > 1$ we add up those two and use the fact that $B_n = B_n^*$ and 
	find the result we want. For $n = 1$, there is just the first contribution 
	and $c_{\xi}(1,1) = - B_1$, which gives
	\begin{equation*}
		B_1^* \ad_{\xi}(\eta)
	\end{equation*}
	in total. For $n = 0$ , we get $c_{\xi}(1) = c_{\eta}(1) = 1$ and finally 
	get \eqref{Alg:BCHFirstOrderXi}. For \eqref{Alg:BCHFirstOrderEta}, note 
	that we need $c_{\xi}(1, n)$ and $c_{\eta}(1, 1, n-1)$. We have 
	$c_{\xi}(1,n) = (-1)^n c_{\eta}(1,n)$ and 
	$c_{\eta}(1, 1, n-1) = (-1)^n c_{\xi}(1,1,n-1$.
	This gives $(-1)^n$ and switches $B_n$ to $B_n^*$.
\end{proof}
\begin{remark}[Alternative Proof]
	Note that we could also have used the integral formula 
	\eqref{Alg:BCHinIntegral} to prove this. We want to sketch an alternative 
	proof here: if we write the second of the two exponential functions as a 
	series, we see that it can be cut after the constant term, since we are 
	looking for contributions which are linear in $\eta$. The function left to 
	integrate is then just $(g \circ \log)(z)$. Since we insert 
	$\exp(\ad_{\xi})$, we get
	\begin{equation*}
		\bch{\xi}{\eta}
		=
		\xi + 
		\int_0^1
		g \left( \ad_{\xi} \right)
		(\eta)
		dt
		+ \mathcal{O} \left( \eta^2 \right)
		=
		\xi +
		\sum\limits_{n = 1}^{\infty}
		\frac{B_n^*}{n!}
		\left( \ad_{\xi} \right)^n (\eta)
		+ \mathcal{O} \left( \eta^2 \right),
	\end{equation*}
	since there is no dependence on $t$ left and we get the same result.
\end{remark}


\subsubsection{Thompson's estimates on the coefficients}
We know that we can put the BCH series into the form of Equation 
\eqref{Alg:BCHinGoldbergThompson}:
\begin{equation}
	\bch{\xi}{\eta}
	=
	\sum\limits_{n = 1}^{\infty}
	\sum\limits_{|w| = n}
	\frac{g_w}{n} [w].
\end{equation}
We will need estimates on the coefficients $g_w$ in order to show the 
continuity of the Gutt star product. The first simple estimate (together with a 
table of the first Goldberg coefficients and good explanation of Goldberg 
polynomials) was given by Newman and Thompson in \cite{newman.thompson:1987a}. 
The idea behind was an analysis of the structure of the polynomials an its 
roots. This allows to put tight bounds on the values of $|g_w|$. We will need a 
bit more than that, but luckily, Thompson gave an estimate in exactly the form 
we will need in \cite{thompson:1989a}.
\begin{proposition}
	\label{Alg:Prop:ThompsonsEstimate}
	Let $n \in \mathbb{N}$ and $g_w$ denotes the Goldberg coefficient of a 
	word in two letters. Then we have the estimate
	\begin{equation}
		\label{Alg:ThompsonsEstimate}
		\sum\limits_{|w| = n}
		\left| g_w \right|
		\leq
		2
	\end{equation}
\end{proposition}
\begin{proof}
	We want to sketch the proof here for convenience. One can see from the 
	recursion formula \eqref{Alg:GoldbergPolynomials}, that the $G_s(t)$ are 
	symmetric around $z = \frac{1}{2}$ (maybe up to a factor $t - \frac{1}
	{2}$): doing a shift $t \mapsto t \frac{1}{2}$, they are of the form
	\begin{equation*}
		G_s(t)
		=
		\frac{1}{s!}
		\frac{d}{dt}(t^2 - \frac{1}{4})
		\cdots
		\frac{d}{dt}(t^2 - \frac{1}{4}).
	\end{equation*}
	Their roots lie all in the interval $(0,1)$ and the polynomials are normed 
	(which can also be seen from the recursion formula). This means that they 
	can be written (in the shifted form) as
	\begin{equation*}
		G_s(t)
		=
		t^{s_0}
		(t + t_{s_1})(t - t_{s_1})
		\cdots
		(t + t_{s_r})(t - t_{s_r})
	\end{equation*}
	with $r = \lfloor \frac{s-1}{2} \rfloor$, $t_{s_i} \in (0, \frac{1}{2})$ 
	and 	$s_0 = 0$ if $s$ is odd and $s_0 = 1$ if $s$ is even. The symmetric, 
	quadratic terms are bounded in their absolute value by $\frac{1}{4}$ and 
	the linear term by $\frac{1}{2}$, since $t \in (-\frac{1}{2}, \frac{1}
	{2})$. Hence we have the estimate $|G_s(t)| \leq 2^{-s + 1}$ for the 
	integral domain. Now note that integration by parts yields
	\begin{equation*}
		\int\limits_{0}^1
		t^a (t-1)^b
		dt
		=
		\frac{a! b!}{(a + b + 1)!}.
	\end{equation*}
	We can put this together to get an estimate for $g_w = c_{\xi, \eta}(s_1, 
	\ldots, s_m)$. Note again by $n = s_1 + \cdots + s_m$. A slight 
	rearranging of the factor from integration by 
	parts using the fact that one of the numbers $m' = \frac{m}{2}$ and $m'' = 
	\frac{m - 1}{2}$ is not an integer and will therefore be rounded downwards 
	gives
	\begin{align*}
		|g_w|
		& \leq
		2^{-n + m}
		\frac{m' ! m'' !}{(m' + m'' + 1)!}
		\\
		& =
		2^{-n + m}
		\frac{1}{m}
		\binom{m-1}{m'}^{-1}.
	\end{align*}
	Now we just need to sum all the expressions corresponding corresponding to 
	words of the length $n$. Note, that the words can start with $\xi$ or 
	$\eta$ and we therefore get a factor $2$ in front. The number of possible 
	arrangements $(s_1, \ldots, s_m)$ is due to a combinatorial argument 
	($n - m$ balls into $m$ buckets, since every bucket must contains at least 
	one ball) given by $\binom{n-1}{m-1}$ and we have to sum over all possible	
	$m$. We get
	\begin{align*}
		\sum\limits_{|w| = n}
		|g_w|
		& \leq
		\sum\limits_{m = 1}^n
		2 \binom{n-1}{m-1}
		2^{-n + m} \frac{1}{m}
		\binom{m-1}{m'}^{-1}
		\\
		& =
		2^{-n + 1}
		\sum\limits_{m = 1}^n
		\binom{n-1}{m-1}
		\underbrace{
			2^m
			\frac{1}{m}
			\binom{m-1}{m'}^{-1}
		}_{ \leq 2 \ (*)}
		\\
		& \leq
		2^{-n + 2}
		\sum\limits_{m = 1}^n
		\binom{n-1}{m-1}
		\\
		& =
		2.
	\end{align*}
	At $(*)$ we used the fact that $\binom{m-1}{m'}$ is the biggest term (or 
	one of the two) biggest terms in the binomial expansion of $(1 + 1)^{m-1}$ 
	and hence $m \binom{m-1}{m'} \geq 2^{m-1}$. Hence, the statement is 
	proven.
\end{proof}



\section{The Equality of the Star Products}
\label{sec:chap3_StarProductProof}

We want to prove the equality of the three star products. For a general and 
possibly infinite-dimensional Lie algebra, this is quite tedious. As a first 
step, it will be helpful to show their associativity.
\begin{remark}
	In the finite-dimensional case, there are different proofs for 
	Theorem~\ref{Alg:Thm:ThreeStarsAreOne}, the main theorem of this section, 
	which mostly rely on geometric arguments, like the one in 
	\cite{bordemann.neumaier.waldmann:1999a}. Unluckily, these techniques are 
	not at hand in infinite dimensions and one has to find an algebraic proof 
	instead. Since in the community of deformation quantization, this 
	statement is somehow folklore and believed for any Lie algebra, it 
	strongly seems like such a proof already exists. However, the author was 
	not able to trace it down in literature and therefore gives an own proof.
\end{remark}
\begin{proposition}
	\label{Alg:Lemma:Associtivity}
	The three maps $\star_z$, $\widehat{\star}_z$ and $\ast_z$ define
	associative multiplications.
\end{proposition}
\begin{proof}
  All maps are defined as $\Sym^{\bullet}(\lie{g}) \times \Sym^{\bullet}
  (\lie{g}) \longrightarrow \Sym^{\bullet}(\lie{g})$, so we have to show 
  bilinearity and associativity.
  \begin{enumerate}
	\item 
	For $\widehat{\star}_z$, associativity and bilinearity are clear from the 
	construction, since we just pull-back the multiplication in $\algebra{U}
	(\lie{g}_z)$.
	
	\item
	For $\star_z$, bilinearity follows from the fact that all maps, that are 
	used in its definition, are (bi-)linear. For associativity, we have to 
	interchange sums and shift projections.
	Recall that $\pi_n(f \star_z g) = 0$, if $n > \deg(f) + \deg(g)$.
	Take homogeneous tensors $f, g, h \in \Sym^{\bullet}(\lie{g})$
	of degree $k, \ell, m \in \mathbb{N}$ respectively. The we have
	\begin{align*}
		\big(
			f 
		&
			\star_z g
		\big)
		\star_z
		h
		\\
		& =
		\sum\limits_{j=0}^{k + \ell -1}
		\sum\limits_{i = 0}^{k + \ell + m -j - 1}
		z^i
		\left( 
			\pi_{k + \ell + m - j - i} 
			\circ 
			\mathfrak{q}^{-1}
		\right)
		\left(
			\mathfrak{q}
			\left(
				z^j
				\left(
					\pi_{k + \ell - j} 
					\circ 
					\mathfrak{q}^{-1}
				\right)
				\left(
					\mathfrak{q} (f) 
					\odot
					\mathfrak{q} (g)
				\right)
			\right)
			\odot
			\mathfrak{q}(h)
		\right)
		\\
		& =
		\sum\limits_{j=0}^{k + \ell -1}
		\sum\limits_{i = 0}^{k + \ell + m - 1}
		z^{i-j}
		\left( 
			\pi_{k + \ell + m - i} 
			\circ 
			\mathfrak{q}^{-1}
		\right)
		\left(
			\mathfrak{q}
			\left(
				z^j
				\left(
					\pi_{k + \ell - j} 
					\circ 
					\mathfrak{q}^{-1}
				\right)
				\left(
					\mathfrak{q} (f)
					\odot
					\mathfrak{q} (g)
				\right)
			\right)
			\odot
			\mathfrak{q}(h)
		\right)
		\\
		& =
		\sum\limits_{i = 0}^{k + \ell + m - 1}
		z^i
		\left( 
			\pi_{k + \ell + m - i} 
			\circ 
			\mathfrak{q}^{-1}
		\right)
		\left(
			\mathfrak{q}
			\left(
				\sum\limits_{j=0}^{k + \ell -1}
				z^{-j}
				z^j
				\left(
					\pi_{k + \ell - j} 
					\circ 
					\mathfrak{q}^{-1}
				\right)
				\left(
					\mathfrak{q} (f)
					\odot
					\mathfrak{q} (g)
				\right)
			\right)
			\odot
			\mathfrak{q}(h)
		\right)
		\\
		& =
		\sum\limits_{i = 0}^{k + \ell + m - 1}
		z^i
		\left( 
			\pi_{k + \ell + m - i} 
			\circ 
			\mathfrak{q}^{-1}
		\right)
		\left(
			\mathfrak{q}(f)
			\odot
			\mathfrak{q}(g)
			\odot
			\mathfrak{q}(h)
		\right),
	\end{align*}
	and we just need to do the reversed process on the right hand side to get 
	the wanted result.
	
	\item
	For $\ast_z$, we get associativity using the exponential function and the 
	logarithm. We have
	\begin{align*}
		\left( 
			\exp(\xi) \ast_z \exp(\eta) 
		\right) 
		\ast_z \exp(\chi)
		&=
		\exp
		\left(
			\frac{1}{z}
			\bch{
				\left(
					\frac{1}{z}
					\bch{z \xi}{z \eta}
				\right)
			}
			{z \chi}
		\right)
		\\
		&=
		\exp
		\left(
			\frac{1}{z}
			\bch{z \xi}
			{
				\left(
					\frac{1}{z}
					\bch{z \eta}{z \chi}
				\right)
			}
		\right)
		\\
		&=
		\exp(\xi)
		\ast_z
		\left( 
			\exp(\eta) \ast_z \exp(\chi) 
		\right), 
	\end{align*}
	since
	\begin{align*}
	\bch{
		\left(
			\frac{1}{z}
			\bch{z \xi}{z \eta}
		\right)
	}
	{z \chi}
	& =
	\log\left(
		\exp\left(
			\log\left(
				\frac{1}{z}
				\exp\left(
					z \xi
				\right)
				\exp\left(
					z \eta
				\right)
			\right)
		\right)
		\exp\left(
			z \chi
		\right)
	\right)
	\\
	& =
	\log\left(
		\frac{1}{z}
		\exp\left(
			z \xi
		\right)
		\exp\left(
			z \eta
		\right)
		\exp\left(
			z \chi
		\right)
	\right)
	\\
	& =
	\log\left(
		\exp\left(
			z \xi
		\right)
		\log\left(
			\left(
				\frac{1}{z}
				\exp\left(
					z \eta
				\right)
				\exp\left(
					z \chi
				\right)
			\right)
		\right)
	\right)
	\\
	& =
	\bch{z \xi}
	{
		\left(
			\frac{1}{z}
			\bch{z \eta}{z \chi}
		\right)
	}.
	\end{align*}
	Bilinearity follows from differentiating the formula and is 
	a simple computation.
  \end{enumerate}	 
\end{proof}
Note star products must fulfil the classical and the semi-classical limit. 
We will do this in Corollary~\ref{Formulas:Cor:LimitCases} and so just the 
equality is left to show. It is enough to prove the coincidence for terms of 
the form $\xi^k \star \eta$ with $\xi, \eta \in \lie{g}$ and $k \in 
\mathbb{N}$, because $\Sym^{\bullet}(\lie{g})$ is a commutative algebra and 
hence we get them on arbitrary monomials by polarization. The equality for the 
product of two monomials then follows by iteration, which is possible due to 
associativity. The next lemma will be a first big step.
\begin{lemma}
	Let $\xi, \eta \lie{g}$, then we have
	\begin{equation}
		\label{Alg:GuttStarFormula}
		\xi^k \widehat{\star}_z \eta
		=
	 	\sum\limits_{n=0}^k 
	 	z^n \binom{k}{n} B_n^*
	 	\xi^{k-n} 
	 	\left( \ad_{\xi} \right)^n (\eta).
	\end{equation}
\end{lemma}
\begin{proof}
	This proof is divided into the two following lemmata:
	\begin{lemma}
		\label{Alg:Lemma:BadSublemma1}
		Let $\xi, \eta \in \mathfrak{g}$ and $k\in \mathbbm{N}$. Then we have
		\begin{equation*}
			\lie{q}_z \left(\sum\limits_{n=0}^k z^n
			\binom{k}{n} B_n^* \xi^{k-n}
			\left(\ad_{\xi}\right)^n(\eta)\right)
			=
			\sum\limits_{s=0}^k\mathcal{K}(k,s)
			\xi^{k-s} \odot \eta \odot \xi^s
		\end{equation*}
		with
		\begin{equation*}
			 \mathcal{K}(k,s)
			 =
			 \frac{1}{k + 1} \sum\limits_{n=0}^k
			 \binom{k+1}{n} B_n^*
			 \sum\limits_{j=0}^n
			 (-1)^j \binom{n}{j}
			 \sum\limits_{\ell=0}^{k-n} \delta_{s, \ell+j}.
		\end{equation*}
	\end{lemma}
	\begin{subproof}
		Since the map $\mathfrak{q}_z$ is linear, we can pull out the
		constants and get
		\begin{equation*}
			\mathfrak{q}_z \left( \sum\limits_{n=0}^k
			z^n \binom{k}{n} B_n^* \xi^{k-n} 
			\left(
				\ad_{\xi}\right)^n(\eta)
			\right)
			=
			\sum\limits_{n=0}^k
			\binom{k}{n} B_n^*
			\mathfrak{q}_z
			\left(
				z^n \xi^{k-n}
				\left( \ad_{\xi} \right)^n
				(\eta)
			\right).
		\end{equation*}
		Now we need the two equalities
		\begin{align*}
			\mathfrak{q}_z
			\left( \xi^n \eta \right)
			& = 
			\frac{1}{n + 1}
			\sum\limits_{\ell = 0}^n 
			\xi^{k - \ell} \odot \eta \odot \xi^{\ell}
		\intertext{and}
			\mathfrak{q}_z
			\left(
				\left( z^n \ad_{\xi} \right)^n 
				(\eta) 
			\right)
			& = 
			\sum\limits_{j=0}^n
			(-1)^j \binom{n}{j}
			\xi^{n-j} \odot \eta \odot \xi^j
		\end{align*}
		which can easily be shown by induction. They give
		\begin{align*}
			\sum\limits_{n=0}^k
			\binom{k}{n} B_n^*
			&
			\mathfrak{q}_z
			\left(
				z^n \xi^{k-n}
				\left( \ad_{\xi} \right)^n
				(\eta)
			\right)
			\\
			& = 
			\sum\limits_{n=0}^k
			\binom{k}{n} \frac{B_n^*}{k-n+1}
			\sum\limits_{\ell=0}^{k-n}
			\xi^{k-n-\ell} \odot
			\left(
				\sum\limits_{j=0}^n
				(-1)^j \binom{n}{j} 
				\xi^{n-j} \odot \eta \odot \xi^j
			\right) 
			\odot \xi^{\ell}
			\\
			& =
			\sum\limits_{n=0}^k
			\binom{k}{n} \frac{B_n^*}{k-n+1}
			\sum\limits_{\ell=0}^{k-n}
			\sum\limits_{j=0}^n
			(-1)^j \binom{n}{j} 
			\xi^{k-\ell-j} \odot \eta \odot \xi^{\ell+j} 
			\\	
			& = 
			\frac{1}{k + 1} \sum\limits_{n=0}^k
			\binom{k+1}{n} B_n^*
			\sum\limits_{j=0}^n (-1)^j \binom{n}{j}
			\sum\limits_{\ell=0}^{k-n}
			\xi^{k-\ell-j} \odot \eta \odot \xi^{\ell+j}
		\end{align*}
		We just need to collect those terms for which we have $\ell + j = s$
		for all	$s = 0, \ldots, k$. If we do this with a Kronecker-delta, we 
		will get exactly the $\mathcal{K}(k, s)$.
	\end{subproof}
	For the second lemma, we need some statements on Bernoulli numbers and 
	binomial coefficients. Let $k,m,n \in \mathbbm{N}$. Then we have the 
	following identities:
	\begin{align}
		\label{Alg:Lemmastuff:1_B1}
		\sum\limits_{j=0}^k
		\binom{k+1}{n} B_j^* 
		& = k+1 \\
		\label{Alg:Lemmastuff:1_Carl}
		(-1)^k \sum\limits_{j=0}^k
		\binom{k}{j} B_{m+j} 
		& = 
		(-1)^m \sum\limits_{i=0}^m 
		\binom{m}{i} B_{k+i} \\
		\label{Alg:Lemmastuff:1_Bin1} 
		\sum\limits_{j=0}^m
		(-1)^j \binom{n}{j} 
		& = 
		(-1)^m \binom{n-1}{m} \\
		\label{Alg:Lemmastuff:1_Bin2}
		\binom{n}{m}\binom{m}{k}
		& =
		\binom{n}{k}\binom{n-k}{m-k}.
	\end{align}
	The first one can easily be proven using the recursive definition of the 
	Bernoulli numbers \eqref{Alg:BernoulliRecursive}.
	Equation~\eqref{Alg:Lemmastuff:1_Bin1} and \eqref{Alg:Lemmastuff:1_Bin2} 
	are standard identities in combinatorics and can be found in the textbook 
	of Aigner \cite{aigner:2000a}. Finally, 
	Equation~\eqref{Alg:Lemmastuff:1_Carl} is a theorem due to Carlitz 
	\cite{carlitz:1968a}. With them, we can show the next lemma which will 
	finish this proof.
	\begin{lemma}
		\label{Alg:Lemma:BadSublemma2}
		Let $\mathcal{K}(k,s)$ be defined as in Lemma 
		\ref{Alg:Lemma:BadSublemma1}, then we have for all $k\in \mathbbm{N}$
		\begin{equation*}
			\mathcal{K}(k, s)
			=
			\begin{cases}
				1 & s=0 \\
				0 & \text{else}.		
			\end{cases}
		\end{equation*}
	\end{lemma}
	\begin{subproof}
		This is divided into three parts. First, we show the statement for 
		$s=0$, then we show it for $s=1$ and then proceed by induction.
		\begin{enumerate}[(i)]
		  \item $s=0$:
			The Kronecker-delta will always be zero unless $l=j=0$. So we get
			\begin{equation*}
				\mathcal{K}(k,0)
				=
				\frac{1}{k+1} \sum\limits_{n=0}^k
				\binom{k+1}{n} B_n^*
				=
				\frac{k+1}{k+1}
				=
				1,
			\end{equation*}
			where we have used \eqref{Alg:Lemmastuff:1_B1}.

			\item $s=1$:
			To get a contribution from the $\delta$, we must have 
			$(j, \ell) = (1,0)$ or $(0,1)$. Except for $n=0$ and $n=k$, 
			both cases are possible. We split them off:
			\begin{align*}
				\mathcal{K}(k,1)
				& = 
				\underbrace{\frac{1}{k+1}}_{n=0}
				- \underbrace{k B_k^*}_{n=k}
				+ \frac{1}{k+1}
				\sum\limits_{n=1}^{k-1}
				\binom{k+1}{n}B_n^*
				\left(1 + (-1) \binom{n}{1} \right)\\
				& = 
				\frac{1}{k+1}
				+ \frac{1}{k+1} \sum\limits_{n=1}^{k-1}
				\binom{k+1}{n}B_n^* 
				- \frac{1}{k+1}	\sum\limits_{n=1}^{k-1}
				\binom{k+1}{n}n B_n^* 
				- k B_k^* \\
				& = 
				\underbrace{
					\frac{1}{k+1}
					\sum\limits_{n=0}^{k-1}
					\binom{k + 1}{n}
					B_n^*
				}_{ = 1 - B_k^*} 
				- \frac{1}{k+1} \sum\limits_{n=0}^k
				\binom{k+1}{n}n B_n^*
				\\
				& =
				1 - B_k^* 
				- \underbrace{\frac{k+1}{k+1}
				\sum\limits_{n=0}^k
				\binom{k+1}{n} B_n^*}_{ = k+1}
				+ \sum\limits_{n=0}^k 
				\underbrace{\frac{k+1-n}{k+1}
				\binom{k+1}{n}}_{\binom{k}{n}} B_n^*
				\\
				& = 
				1 - B_k^* - k - 1 
				+ \sum\limits_{n=0}^{k-1}
				\binom{k}{n} B_n^* + B_k^*
				\\
				& =
				- k + 
				\sum\limits_{n}^{k-1} 
				\binom{k}{n} B_n^*
				\\
				& = 
				0.
			\end{align*}
	
		  \item $s \mapsto s+1$:
			Due to the induction, it is sufficient to prove 
			$\mathcal{K}(k,s+1) - \mathcal{K}(k,s) = 0$.
			In order to do that, we must get rid of the $\delta$'s and 
			therefore rewrite $\mathcal{K}(k,s)$:
			\begin{align*}
				\mathcal{K}(k,s)
				& = 
				\frac{1}{k + 1}	\sum\limits_{n=0}^k
				\binom{k+1}{n} B_n^* \sum\limits_{j=0}^n
				(-1)^j \binom{n}{j} \sum\limits_{\ell=0}^{k-n}
				\delta_{s, \ell+j}\\
				& = 
				\frac{1}{k + 1} \sum\limits_{n=0}^s
				\binom{k+1}{n} B_n^* \sum\limits_{j=0}^n
				(-1)^j \binom{n}{j} \sum\limits_{\ell=0}^{k-n}
				\delta_{s, \ell+j} \\
				& \quad + \frac{1}{k + 1} \sum\limits_{n=s+1}^k
				\binom{k+1}{n} B_n^* \sum\limits_{j=0}^s
				(-1)^j \binom{n}{j} \sum\limits_{\ell=0}^{k-n}
				\delta_{s, \ell+j} \\
				& = \frac{1}{k + 1} \sum\limits_{n=0}^s
				\binom{k+1}{n} B_n^* 
				\sum\limits_{j=\max\{0, s+n-k\}}^n
				(-1)^j \binom{n}{j} \\
				& \quad + \frac{1}{k + 1}
				\sum\limits_{n=s+1}^k
				\binom{k+1}{n} B_n^*
				\sum\limits_{j=\max\{0, s+n-k\}}^s
				(-1)^j \binom{n}{j}.
			\end{align*}
			As long as $\max\{0, s+n-k\} = 0$, the first sum over $j$ will be 
			zero as it is just the binomial expansion of $(1-1)^n$, except for 
			$n = 0$. Hence we get a special case and a shorter first sum over 
			$n$. In the sums over $j$ we use again the binomial expansion of 
			$(1-1)^n$ and get 
			\begin{align*}
				\mathcal{K}(k,s)
				& = 
				\frac{1}{k+1} \left[ 
				1 + \sum\limits_{k+1-s}^s 
				\binom{k+1}{n} B_n^* \left(
				- \sum\limits_{j=0}^{s+n-k-1} 
				(-1)^j \binom{n}{j}\right) \right. \\
				& \quad + \left. 
				\sum\limits_{n=s+1}^k 
				\binom{k+1}{n} B_n^* \left( 
				- \sum\limits_{j=0}^{s+n-k-1} 
				(-1)^j\binom{n}{j} - 
				\sum\limits_{j=s+1}^n 
				(-1)^j \binom{n}{j} \right) \right].
			\end{align*}
			Now it is helpful to use \eqref{Alg:Lemmastuff:1_Bin1} and 
			$\binom{k}{n-k} = \binom{k}{n}$. We also get $(-1)^n$-terms which 
			we can put together with the $B_n^*$ to get $B_n$:
			\begin{align*}
				\mathcal{K}(k,s)
				& = 
				\frac{1}{k+1} \left[
				1+ \sum\limits_{n=k+1-s}^s
				\binom{k+1}{n} B_n (-1)^{k-s}
				\binom{n-1}{k-s} \right. \\
				& \quad + \left.
				\sum\limits_{n=s+1}^k
				\binom{k+1}{n}B_n \left(
				(-1)^{k-s}\binom{n-1}{k-s} + (-1)^{n+s}\binom{n-1}{s}
				\right) \right].
			\end{align*}
			We finally made the $\delta$ disappear. Hence we must compute
			$\mathcal{K}(k,s+1) - \mathcal{K}(k,s)$. Since we want to show 
			that it is $0$, we can multiply it with $k + 1$ in order to get 
			rid of the factor in front:
			\begin{align*}
				& \quad
				(k+1) \left( \mathcal{K}(k,s+1) - \mathcal{K}(k,s) \right)\\
				& =
				\sum\limits_{n=k-s}^{s+1}
				\binom{k+1}{n} B_n (-1)^{k-s-1}
				\binom{n-1}{k-s-1} - \sum\limits_{n=k+1-s}^s
				\binom{k+1}{n} B_n (-1)^{k-s} \binom{n-1}{k-s} \\
				& \quad
				+ \sum\limits_{n=s+2}^k
				\binom{k+1}{n} B_n \left(
				(-1)^{k-s-1}\binom{n-1}{k-s-1}
				+ (-1)^{n+s+1}\binom{n-1}{s+1}
				\right) \\
				& \quad
				- \sum\limits_{n=s+1}^k
				\binom{k+1}{n} B_n \left(
				(-1)^{k-s}\binom{n-1}{k-s} + (-1)^{n+s}\binom{n-1}{s}
				\right) \\
				& =
				-\sum\limits_{n=k-s}^k
				\binom{k+1}{n} B_n (-1)^{k-s}
				\binom{n-1}{k-s-1} -\sum\limits_{n=k-s+1}^k
				\binom{k+1}{n}B_n (-1)^{k-s} \binom{n-1}{k-s} \\
				& \quad -
				\sum\limits_{n=s+2}^k
				\binom{k+1}{n}B_n (-1)^{n+s}
				\binom{n-1}{s+1} - \sum\limits_{n=s+1}^k
				\binom{k+1}{n}B_n (-1)^{n+s} \binom{n-1}{s} \\
				& =
				-\sum\limits_{n=k-s}^k
				\binom{k+1}{n}B_n (-1)^{k-s} \left(
				\binom{n-1}{k-s-1} + \binom{n-1}{k-s}
				\right) \\
				& \quad -
				\sum\limits_{n=s+1}^k
				\binom{k+1}{n} B_n (-1)^{n+s}
				\left( \binom{n-1}{s+1} + \binom{n-1}{s} \right).
			\end{align*}
			We have rearranged the sums, added some zeros and shortened the 
			expression. Now we will use the recursion formula for the binomial 
			coefficients
			\begin{equation*}
				\binom{n-1}{k-1} + \binom{n-1}{k}
				=
				\binom{n}{k}
			\end{equation*}
			and our binomial multiplication equality 
			\eqref{Alg:Lemmastuff:1_Bin2}:
			\begin{align*}
				& =
				- \sum\limits_{n=k-s}^k
				\binom{k+1}{n} B_n (-1)^{k-s} \binom{n}{k-s}
				- \sum\limits_{n=s+1}^k
				\binom{k+1}{n} B_n (-1)^{n+s} \binom{n}{s+1} \\
				& = 
				- \sum\limits_{n=k-s}^k 
				\binom{k+1}{s+1} \binom{s+1}{n+s-k} B_n (-1)^{k-s}
				- \sum\limits_{n=s+1}^k
				\binom{k+1}{s+1} \binom{k-s}{n-s-1} B_n (-1)^{n+s}.
			\end{align*}
			Since we want to show that this is $0$, we can divide by
			$\binom{k+1}{s+1}$ which will never be zero because 
			$s \in \{0, 1, \ldots, k\}$. After doing so, can use $n > 1$ in 
			the second sum and thus only even $n$ will show up, because for 
			odd $n$ the Bernoulli numbers are zero. For this reason we have 
			$(-1)^n = 1$. Then we rewrite these sums by shifting the indices 
			and we add two zeros:
			\begin{align*}
				& \quad
				- \sum\limits_{n=k-s}^k
				\binom{s+1}{n+s-k} B_n (-1)^{k-s}
				+ \sum\limits_{n=s+1}^k
				\binom{k-s}{n-s-1} B_n (-1)^{s+1} \\
				& =
				(-1)^{s+1} \sum\limits_{\ell=0}^{k-s-1}
				\binom{k-s}{\ell} B_{\ell+s+1}
				- (-1)^{k-s}\sum\limits_{\ell=0}^s 
				\binom{s+1}{\ell} B_{\ell + k - s} \\
				& =
				(-1)^{s+1} \sum\limits_{\ell=0}^{k-s}
				\binom{k-s}{\ell} B_{\ell+s+1}
				- (-1)^{k-s} \sum\limits_{\ell=0}^{s+1}
				\binom{s+1}{\ell} B_{\ell + k - s}\\
				& \quad
				- (-1)^{s+1} \binom{k-s}{k-s} B_{k+1} 
				+ (-1)^{k-s} \binom{s+1}{s+1} B_{k+1}.
			\end{align*}
			The first two terms give the Carlitz-identity 
			\eqref{Alg:Lemmastuff:1_Carl} and vanish. So we are left with 
			the last two terms and get
			\begin{equation*}
				- (-1)^{s+1} B_{k+1} + (-1)^{k-s} B_{k+1}
				=
				(-1)^s B_{k+1} \left(1 + (-1)^k\right) = 0,
			\end{equation*}
			since the bracket will be zero if $k$ is odd and $B_{k+1}=0$ if 
			$k$ is even.
		\end{enumerate}
	\end{subproof}
\end{proof}
In Lemma \ref{Formulas:Lemma:LinearMonomial1}, we will see that also	$\ast_z$ 
fulfils this identity. Hence $\ast_z = \widehat{\star}_z$. We only need to 
show $\widehat{\star}_z = \star_z$. For $z = 1$, the two 	maps are clearly 
identical and therefore we find
\begin{equation*}
	\xi^k \star_1 \eta
	=
	\sum\limits_{n=0}^k
	\binom{k}{n} B_n^* \xi^{k-n}
	\left( \ad_{\xi} \right)^k(\eta).
\end{equation*}
But now $\widehat{\star}_z = \star_z$ follows from the definition of $\star_z$: 
we just have to plug in powers of $z$ and find \eqref{Alg:GuttStarFormula}. 
So with the proofs in Chapter 4, we will have proven the following theorem:
\begin{theorem}
	\label{Alg:Thm:ThreeStarsAreOne}
	The three maps $\star_z$, $\widehat{\star}_z$ and $\ast_z$ 
	coincide on $\Sym^{\bullet}(\lie{g})$ and define star products.
\end{theorem}

